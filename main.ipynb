{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiimkhang/dog_breed_classification/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get dataset"
      ],
      "metadata": {
        "id": "HUtEtxGzXjJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = \"dog_breed_classification/small_dataset.zip\"\n",
        "if not os.path.isdir('small_dataset'):\n",
        "    !git clone https://github.com/hiimkhang/dog_breed_classification\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n"
      ],
      "metadata": {
        "id": "IOx9I8nYEPrq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customize ResNet50 module"
      ],
      "metadata": {
        "id": "UIZxOhxwXp5Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "eXeGmCEXENKL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from typing import Type, Any, Callable, Union, List, Optional\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "\n",
        "\n",
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        #out += identity\n",
        "        out = self.skip_add.add(out, identity)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion: int = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        #out += identity\n",
        "        out = self.skip_add.add(out, identity)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        layers: List[int],\n",
        "        num_classes: int = 1000,\n",
        "        zero_init_residual: bool = False,\n",
        "        groups: int = 1,\n",
        "        width_per_group: int = 64,\n",
        "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        self.quant = QuantStub()\n",
        "        self.dequant = DeQuantStub()\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n",
        "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.quant(x) # add quant\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        x = self.dequant(x) # add dequant\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(\n",
        "    arch: str,\n",
        "    block: Type[Union[BasicBlock, Bottleneck]],\n",
        "    layers: List[int],\n",
        "    pretrained: bool,\n",
        "    progress: bool,\n",
        "    **kwargs: Any\n",
        ") -> ResNet:\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50_quantizable(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    r\"\"\"ResNet-50 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n",
        "                   **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare dataloader"
      ],
      "metadata": {
        "id": "LbfAie6sXxxt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "pw9jrTJEENKP"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "\n",
        "root = './small_dataset'\n",
        "img_size = 224\n",
        "\n",
        "train_batch_size = 8\n",
        "eval_batch_size = 8\n",
        "num_workers = 2\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                                std=(0.229, 0.224, 0.225)),\n",
        "    ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                                std=(0.229, 0.224, 0.225)),\n",
        "    ])\n",
        "\n",
        "train_set = datasets.ImageFolder(root=os.path.join(root, 'train'), transform=train_transform)\n",
        "test_set = datasets.ImageFolder(root=os.path.join(root, 'valid'), transform=test_transform)\n",
        "\n",
        "train_sampler = torch.utils.data.RandomSampler(train_set)\n",
        "test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_set,\n",
        "                        batch_size=train_batch_size,\n",
        "                        sampler=train_sampler,\n",
        "                        num_workers=num_workers)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_set,\n",
        "                        batch_size=eval_batch_size,\n",
        "                        sampler=test_sampler,\n",
        "                        num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "xnAT7dLMENKP"
      },
      "outputs": [],
      "source": [
        "cuda_device = torch.device(\"cuda:0\")\n",
        "cpu_device = torch.device(\"cpu:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC3NwYzrENKQ",
        "outputId": "e7475d05-5a42-4dae-dbe2-a1cf31d3de72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape: torch.Size([8, 3, 224, 224])\n",
            "Number of classes (out_features): 3\n",
            "in_channels:  3\n"
          ]
        }
      ],
      "source": [
        "batch_inputs, batch_targets = next(iter(train_loader))\n",
        "\n",
        "in_channels = batch_inputs.shape[1]     # number of channels of input images\n",
        "out_features = len(train_set.classes)\n",
        "\n",
        "print(\"Input tensor shape:\", batch_inputs.shape)\n",
        "print(\"Number of classes (out_features):\", out_features)\n",
        "print(\"in_channels: \", in_channels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper training functions\n",
        "- modifyLayers\n",
        "- train_step\n",
        "- eval_step\n",
        "- train_model"
      ],
      "metadata": {
        "id": "PNdQSAZsX-Co"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "OA8dfHW7ENKR"
      },
      "outputs": [],
      "source": [
        "def modifyLayers(model: torch.nn.Module, in_channels: int, out_features: int):\n",
        "    # modify the first convolutional layer, in_channels = input tensor channels\n",
        "    model.conv1 = torch.nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "    # modify the last fc layer, out_features = number of classes\n",
        "    model.fc = torch.nn.Linear(model.fc.in_features, out_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "IXD0BEQ5ENKR"
      },
      "outputs": [],
      "source": [
        "def test(model: torch.nn.Module, loaders: DataLoader):\n",
        "    # Test the model\n",
        "    model.to('cpu')\n",
        "    model.eval()\n",
        "    accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in tqdm(loaders, total=len(loaders)):\n",
        "            images = images.to('cpu')\n",
        "            labels = labels.to('cpu')\n",
        "\n",
        "            # Quantize the input data (images)\n",
        "            # images_quantized = torch.quantization.quantize_data(images, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
        "            test_output = model(images)\n",
        "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
        "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
        "\n",
        "    print(f\"Test Accuracy of the model on the {len(loaders) * loaders.batch_size} test images: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: nn.Module,\n",
        "               train_loader: DataLoader,\n",
        "               loss_fn: nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> tuple[float, float]:\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    train_loss, train_accuracy = 0., 0.\n",
        "\n",
        "    for batch, (image, label) in enumerate(train_loader):\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        label_pred = model(image) # forward pass\n",
        "\n",
        "        loss = loss_fn(label_pred, label) # compute loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad() # zero param gradients\n",
        "\n",
        "        loss.backward() # backward pass\n",
        "\n",
        "        optimizer.step() # update parameters\n",
        "\n",
        "        y_pred_labels = torch.argmax(torch.softmax(label_pred, dim=1), dim=1) # compute accuracy\n",
        "        train_accuracy += (y_pred_labels == label).sum().item() / len(label_pred)\n",
        "\n",
        "    avg_loss, avg_acc = train_loss/len(train_loader), train_accuracy/len(train_loader)\n",
        "    print(f\"Train loss: {avg_loss:.4f} | Train accuracy: {avg_acc:.4f}\", end=' | ')\n",
        "\n",
        "    return avg_loss, avg_acc"
      ],
      "metadata": {
        "id": "QtNduHml6xYe"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_step(model: nn.Module,\n",
        "              test_loader: DataLoader,\n",
        "              loss_fn: nn.Module,\n",
        "              device: torch.device) -> tuple[float, float]:\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_loss, test_accuracy = 0., 0.\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for batch, (image, label) in enumerate(test_loader):\n",
        "            image, label = image.to(device), label.to(device)\n",
        "\n",
        "            label_pred_logits = model(image) # forward pass\n",
        "\n",
        "            loss = loss_fn(label_pred_logits, label) # compute loss\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            label_pred_labels = label_pred_logits.argmax(dim=1) # compute accuracy\n",
        "            test_accuracy += (label_pred_labels == label).sum().item() / len(label_pred_labels)\n",
        "\n",
        "    avg_loss, avg_acc = test_loss/len(test_loader), test_accuracy/len(test_loader)\n",
        "    print(f\"Test loss: {avg_loss:.4f} | Test accuracy: {avg_acc:.4f}\")\n",
        "\n",
        "    return avg_loss, avg_acc"
      ],
      "metadata": {
        "id": "Xk5hrjYr7_9d"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model: nn.Module,\n",
        "          train_loader: DataLoader,\n",
        "          test_loader: DataLoader,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> dict[str, list]:\n",
        "    global results_info\n",
        "    model.to(device)\n",
        "\n",
        "    results = {\n",
        "        'Train loss': [],\n",
        "        'Train accuracy': [],\n",
        "        'Test loss': [],\n",
        "        'Test accuracy': [],\n",
        "    }\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(),\n",
        "                            1e-3,\n",
        "                            momentum=0.9,\n",
        "                            weight_decay=1e-4)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                     milestones=[100, 150],\n",
        "                                                     gamma=0.1,\n",
        "                                                     last_epoch=-1)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}: \", end=\"\")\n",
        "\n",
        "        train_loss, train_acc = train_step(model, train_loader, loss_fn, optimizer, device)\n",
        "        test_loss, test_acc = eval_step(model, test_loader, loss_fn, device)\n",
        "\n",
        "        results['Train loss'].append(train_loss)\n",
        "        results['Train accuracy'].append(train_acc)\n",
        "        results['Test loss'].append(test_loss)\n",
        "        results['Test accuracy'].append(test_acc)\n",
        "\n",
        "        scheduler.step()\n",
        "        # Write model into log.txt\n",
        "        # with open('log.txt', 'w') as f:\n",
        "        #     f.write(f\"Model: {model}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "0V6otwHR8pt5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model loading and training"
      ],
      "metadata": {
        "id": "7fAe_C9ZYO9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# Use an original resnet50 pretrained model to train and evaluate\n",
        "# Can be used for later comparisons\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "\n",
        "modifyLayers(model=resnet50,\n",
        "             in_channels=in_channels,\n",
        "             out_features=out_features) # modify first conv1 and last fc layer according to dataset\n",
        "\n",
        "resnet50_result = train_model(model=resnet50,\n",
        "            train_loader=train_loader,\n",
        "            test_loader=test_loader,\n",
        "            epochs=3,\n",
        "            device=cuda_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diOlDL-dCiTo",
        "outputId": "c45ae317-e461-430a-cb9c-b346202a493a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3: Train loss: 1.0761 | Train accuracy: 0.3995 | Test loss: 1.1252 | Test accuracy: 0.3854\n",
            "\n",
            "Epoch 2/3: Train loss: 0.9122 | Train accuracy: 0.5543 | Test loss: 0.8148 | Test accuracy: 0.5583\n",
            "\n",
            "Epoch 3/3: Train loss: 0.7314 | Train accuracy: 0.6467 | Test loss: 0.9088 | Test accuracy: 0.5812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AWXQDC0ENKS",
        "outputId": "ce757624-e343-4b70-b1e2-8438558bf5bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3: Train loss: 1.0457 | Train accuracy: 0.4484 | Test loss: 0.8763 | Test accuracy: 0.5021\n",
            "\n",
            "Epoch 2/3: Train loss: 0.8534 | Train accuracy: 0.5924 | Test loss: 0.7063 | Test accuracy: 0.6542\n",
            "\n",
            "Epoch 3/3: Train loss: 0.6069 | Train accuracy: 0.7092 | Test loss: 0.8037 | Test accuracy: 0.6292\n"
          ]
        }
      ],
      "source": [
        "# Use customized resnet50 pretrained model to train and evaluate\n",
        "# This quantizable model later will be used to quantize into the compressed int8 param model\n",
        "\n",
        "resnet50_quantizable = resnet50_quantizable(pretrained=True)\n",
        "\n",
        "modifyLayers(model=resnet50_quantizable,\n",
        "             in_channels=in_channels,\n",
        "             out_features=out_features) # modify first conv1 and last fc layer according to dataset\n",
        "\n",
        "resnet50_quantizable_result = train_model(model=resnet50_quantizable,\n",
        "            train_loader=train_loader,\n",
        "            test_loader=test_loader,\n",
        "            epochs=3,\n",
        "            device=cuda_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POzjSsgeENKS",
        "outputId": "a2c131c3-54c0-4789-d1b6-55d600bc7f53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet50: Test loss: 0.9088 | Test accuracy: 0.5812\n",
            "Quantizable ResNet50: Test loss: 0.8037 | Test accuracy: 0.6292\n"
          ]
        }
      ],
      "source": [
        "# Use the original resnet50 and customized quantizable resnet50 to evaluate on the test\n",
        "print(\"ResNet50: \", end=\"\")\n",
        "resnet50_eval = eval_step(resnet50, test_loader, loss_fn, cpu_device)\n",
        "\n",
        "print(\"Quantizable ResNet50: \", end=\"\")\n",
        "resnet50_quantizable_eval = eval_step(resnet50_quantizable, test_loader, loss_fn, cpu_device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the two trained models\n",
        "if not os.path.isdir('save_models'):\n",
        "    os.makedirs('save_models', exist_ok=True)\n",
        "\n",
        "torch.save(resnet50.state_dict(), \"./save_models/resnet50_trained.pt\")\n",
        "torch.save(resnet50_quantizable.state_dict(), \"./save_models/resnet50_quantizable_trained.pt\")"
      ],
      "metadata": {
        "id": "MSNNYCw_IjcC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post-training static quantization"
      ],
      "metadata": {
        "id": "V4fQnC10YT_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calibrate_model(model: nn.Module, loader: DataLoader, device=torch.device('cpu:0')):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  for image, label in tqdm(loader, total=len(loader)):\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    model(image)"
      ],
      "metadata": {
        "id": "k9EfM_NfMjte"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuPBo4GaENKT",
        "outputId": "c74fe044-4dad-41fa-96cd-4c2adc426f77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:32<00:00,  2.75s/it]\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "# Our initial baseline model which is FP32\n",
        "model_fp32 = copy.deepcopy(resnet50_quantizable)\n",
        "\n",
        "# Move the model to CPU since static quantization does not support CUDA currently.\n",
        "model_fp32.to(cpu_device)\n",
        "\n",
        "model_fp32.eval()   # set to evaluation mode, a requirement for below layers fusing\n",
        "\n",
        "# Fuse layers\n",
        "model_fp32 = torch.quantization.fuse_modules(model_fp32, [['conv1', 'bn1', 'relu']], inplace=True)\n",
        "\n",
        "# Set the backend for x86\n",
        "model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "\n",
        "# Prepares the model for calibration.\n",
        "# Inserts observers in the model that will observe the activation tensors during calibration\n",
        "model_fp32_prepared = torch.quantization.prepare(model_fp32, inplace = False)\n",
        "\n",
        "# Calibrate over the train dataset. This determines the quantization params for activation.\n",
        "# Use training data for calibration.\n",
        "calibrate_model(model=model_fp32_prepared,\n",
        "                loader=train_loader,\n",
        "                device=cpu_device)\n",
        "\n",
        "# for batch, target in tqdm(train_loader, total=len(train_loader)):\n",
        "#     model_fp32_prepared(batch)\n",
        "\n",
        "# Converts the model to a quantized model (int8)\n",
        "resnet50_quantized = torch.quantization.convert(model_fp32_prepared)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "ayRlE_yHENKT"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir('save_models'):\n",
        "    os.makedirs('save_models', exist_ok=True)\n",
        "\n",
        "torch.save(resnet50_quantized.state_dict(), \"./save_models/resnet50_quantized.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the compressed quantized model to evaluate the test set\n",
        "resnet50_quantized_eval = eval_step(resnet50_quantized, test_loader, loss_fn, cpu_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzgUj6ujYkoV",
        "outputId": "42c3d51e-dd6c-4a9a-ced2-19bb97f0c64f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.8316 | Test accuracy: 0.6250\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8316468851019939, 0.625)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare to the previous Quantizable ResNet50 and original ResNet50:\n",
        "print(f\"ResNet50 accuracy: {resnet50_eval[1]:.4f}\")\n",
        "print(f\"Quantizable ResNet50 accuracy: {resnet50_quantizable_eval[1]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1rxhWpOiBp2",
        "outputId": "e68c50fa-f20f-4bb4-a32a-467216171efe"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet50 accuracy: 0.5812\n",
            "Quantizable ResNet50 accuracy: 0.6292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "mJXeZyRHXQGc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4SDNIRqXVa5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}